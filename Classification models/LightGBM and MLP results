MLP overall returns a performance of 0.73 accuracy, but obtains no precision, recall and f1 score for class 1 likely due to two things:

(1) that there are too few samples for MLP to properly classify it, therefore classifying all class 1 instances as class 0 

(2) that class 0 is not easily generalizable. may be too sparse even after smote, hence why the "best" model does not classify any of its instances correctly. 

LightGBM was able to obtain a performance of 0.76 accuracy, with precision, recall and accuracy scores for both classes. Class 0 has significantly higher scores 
than class 1, which is likely due to its sparseness and generalizability, causing the model some difficulty in classifying its instances. 

more info in the ipynb file
